{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XlOR-mUKlYvg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import CLIPProcessor, CLIPModel, GPT2Config, GPT2LMHeadModel\n",
        "from pycocotools.coco import COCO\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKjb3fNMlf7O",
        "outputId": "78a6c270-b42c-420b-e97b-c10913401e4e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Download and Load COCO Annotations\n",
        "ann_file = \"captions_train2017.json\"\n",
        "\n",
        "if not os.path.exists(ann_file):\n",
        "    !wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "    !unzip annotations_trainval2017.zip -d .\n",
        "    ann_file = \"annotations/captions_train2017.json\"\n",
        "\n",
        "coco = COCO(ann_file)  # Load COCO captions dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWm6zVRRllp8",
        "outputId": "b8acd99e-dab9-4618-a65a-78519de00590"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-30 19:16:47--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 16.15.177.69, 54.231.170.249, 3.5.21.92, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|16.15.177.69|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: ‘annotations_trainval2017.zip’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.19M  15.7MB/s    in 17s     \n",
            "\n",
            "2025-01-30 19:17:05 (13.8 MB/s) - ‘annotations_trainval2017.zip’ saved [252907541/252907541]\n",
            "\n",
            "Archive:  annotations_trainval2017.zip\n",
            "  inflating: ./annotations/instances_train2017.json  \n",
            "  inflating: ./annotations/instances_val2017.json  \n",
            "  inflating: ./annotations/captions_train2017.json  \n",
            "  inflating: ./annotations/captions_val2017.json  \n",
            "  inflating: ./annotations/person_keypoints_train2017.json  \n",
            "  inflating: ./annotations/person_keypoints_val2017.json  \n",
            "loading annotations into memory...\n",
            "Done (t=0.93s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold=5):\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<BOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {v: k for k, v in self.itos.items()}\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def build_vocab(self, captions):\n",
        "        counter = nltk.FreqDist()\n",
        "        for caption in captions:\n",
        "            tokens = word_tokenize(caption.lower())\n",
        "            counter.update(tokens)\n",
        "\n",
        "        idx = 4\n",
        "        for word, count in counter.items():\n",
        "            if count >= self.freq_threshold:\n",
        "                self.stoi[word] = idx\n",
        "                self.itos[idx] = word\n",
        "                idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in tokens]\n",
        "\n",
        "# ✅ Build Vocabulary from COCO Captions\n",
        "captions = [coco.anns[ann_id][\"caption\"] for ann_id in coco.anns.keys()]\n",
        "vocab = Vocabulary(freq_threshold=5)\n",
        "vocab.build_vocab(captions)  # Now `vocab` is properly initialized\n",
        "\n",
        "print(f\"Vocabulary Size: {len(vocab.stoi)}\")  # ✅ Debugging to confirm vocab size\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tolkhFvZlnf8",
        "outputId": "441f8f44-f79d-4a19-93eb-91005560af1f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 10322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class COCODataset(Dataset):\n",
        "    def __init__(self, ann_file, transform_norm, transform_clip, vocab):\n",
        "        self.coco = COCO(ann_file)\n",
        "        self.ids = list(self.coco.anns.keys())[:5000]  # Subset for faster training\n",
        "        self.transform_norm = transform_norm\n",
        "        self.transform_clip = transform_clip\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ann_id = self.ids[idx]\n",
        "        caption = self.coco.anns[ann_id][\"caption\"]\n",
        "        img_id = self.coco.anns[ann_id][\"image_id\"]\n",
        "        img_data = self.coco.loadImgs(img_id)[0]\n",
        "        img_url = img_data[\"coco_url\"]\n",
        "\n",
        "        response = requests.get(img_url)\n",
        "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "\n",
        "        # ✅ Apply two different transformations: one for ResNet, one for CLIP\n",
        "        image_norm = self.transform_norm(image)  # Normalized for ResNet/GPT\n",
        "        image_raw = self.transform_clip(image)  # Unnormalized for CLIP\n",
        "\n",
        "        numericalized_caption = [self.vocab.stoi[\"<BOS>\"]] + \\\n",
        "                                self.vocab.numericalize(caption) + \\\n",
        "                                [self.vocab.stoi[\"<EOS>\"]]\n",
        "\n",
        "        return image_norm, image_raw, torch.tensor(numericalized_caption)\n",
        "\n",
        "\n",
        "# ✅ Define `collate_fn` to Fix Padding Issue\n",
        "def collate_fn(batch):\n",
        "    images_norm = [item[0] for item in batch]  # ResNet normalized images\n",
        "    images_raw = [item[1] for item in batch]  # Original images for CLIP\n",
        "    captions = [item[2] for item in batch]\n",
        "\n",
        "    images_norm = torch.stack(images_norm, dim=0)  # ResNet input\n",
        "    images_raw = torch.stack(images_raw, dim=0)  # CLIP input\n",
        "    captions = pad_sequence(captions, batch_first=True, padding_value=0)  # Pad captions\n",
        "\n",
        "    return images_norm, images_raw, captions\n",
        "\n",
        "\n",
        "# ✅ Normalized transform (for ResNet/GPT)\n",
        "transform_norm = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# ✅ Raw transform (for CLIP - NO NORMALIZATION)\n",
        "transform_clip = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])  # No normalization for CLIP!\n",
        "\n",
        "# ✅ Initialize Dataset & DataLoader with both transforms\n",
        "train_dataset = COCODataset(ann_file, transform_norm, transform_clip, vocab)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True, collate_fn=collate_fn\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5lczjfJltI8",
        "outputId": "034692e9-8527-42e8-b121-eb27a3fed7f3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=1.46s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "class CLIPFeatureExtractor(nn.Module):\n",
        "    def __init__(self, clip_model, input_dim=512, output_dim=768):\n",
        "        super(CLIPFeatureExtractor, self).__init__()\n",
        "        self.clip_model = clip_model\n",
        "        self.projection = nn.Linear(input_dim, output_dim)  # Convert 512 → 768\n",
        "\n",
        "    def forward(self, images_raw):  # ✅ Use unnormalized images\n",
        "        inputs = clip_processor(images=images_raw, return_tensors=\"pt\", do_rescale=False).to(images_raw.device)\n",
        "        with torch.no_grad():\n",
        "            features = self.clip_model.get_image_features(**inputs)  # (batch_size, 512)\n",
        "\n",
        "        projected_features = self.projection(features)  # (batch_size, 768)\n",
        "        return projected_features.unsqueeze(1)  # Ensure 3D shape (batch, 1, 768)\n",
        "\n",
        "# ✅ Replace direct CLIP extraction with the new class\n",
        "clip_feature_extractor = CLIPFeatureExtractor(clip_model).to(device)"
      ],
      "metadata": {
        "id": "12kDbVtClvaM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=768):\n",
        "        super(DecoderTransformer, self).__init__()\n",
        "\n",
        "        config = GPT2Config.from_pretrained(\"gpt2\")\n",
        "        config.add_cross_attention = True\n",
        "\n",
        "        self.gpt2 = GPT2LMHeadModel(config)\n",
        "        self.gpt2.resize_token_embeddings(vocab_size)\n",
        "\n",
        "    def forward(self, captions, features):\n",
        "        assert features.dim() == 3, f\"Encoder output must be 3D, but got {features.shape}\"\n",
        "        outputs = self.gpt2(input_ids=captions, encoder_hidden_states=features)\n",
        "        return outputs.logits\n"
      ],
      "metadata": {
        "id": "3jv2hmASlxNd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "embed_size = 768\n",
        "vocab_size = len(vocab.stoi)\n",
        "\n",
        "decoder = DecoderTransformer(vocab_size, embed_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
        "optimizer = optim.AdamW(decoder.parameters(), lr=1e-4)\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    decoder.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for images_norm, images_raw, captions in tqdm(train_loader):\n",
        "        images_norm, images_raw, captions = images_norm.to(device), images_raw.to(device), captions.to(device)\n",
        "\n",
        "        # ✅ Extract features using CLIP with proper projection\n",
        "        features = clip_feature_extractor(images_raw)  # Now correctly 768-dimensional\n",
        "\n",
        "        input_captions = captions[:, :-1]\n",
        "        target_captions = captions[:, 1:]\n",
        "\n",
        "        outputs = decoder(input_captions, features)\n",
        "        loss = criterion(outputs.view(-1, outputs.size(-1)), target_captions.reshape(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpRsfz2mlz5V",
        "outputId": "5a8010ac-351f-4629-9d9a-fdf9f02b108b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [49:37<00:00,  9.51s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 4.5011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 16/313 [02:33<39:05,  7.90s/it]"
          ]
        }
      ]
    }
  ]
}